# =====================
# GRAPH INFORMATION PATHS
# =====================
dirs:
  logits: "results/fact_mj_bball/logits"
  attribute_graph: "results/fact_mj_bball/attribute.json"
  neuronpedia_graph: "results/fact_mj_bball/neuronpedia.json"
  cache: "results/gemma2b_transcoder-sparsify-1m_cache"
  output: "results/fact_mj_bball/experiment_1"

# =====================
# SAMPLER CONFIGURATIONS
# =====================
sampler_configs:
  default:
    n_examples_train: 10
    n_examples_test: 0
    n_quantiles: 10
    train_type: "quantiles"
    test_type: "quantiles"

# =====================
# CONSTRUCTOR CONFIGURATIONS
# =====================
constructor_configs:
  default:
    example_ctx_len: 16
    min_examples: 10
    n_non_activating: 0
    center_examples: true
    non_activating_source: "random"

# =====================
# EXPLAINER CONFIGURATIONS
# =====================
explainers:
  default:
    threshold: 0.3
    verbose: true
    activations: true
    cot: false
    temperature: 0.0

  graph:
    threshold: 0.3
    verbose: true
    activations: true
    cot: true
    temperature: 0.0
    max_parent_explanations: 2
    max_examples: 20

# =====================
# CLIENT CONFIGURATIONS
# =====================
client_configs:
  offline:
    type: "offline"
    max_memory: 0.8
    max_model_len: 4096
    num_gpus: "auto"

  offline_limited:
    type: "offline"
    max_memory: 0.6
    max_model_len: 2048
    num_gpus: 1

  openrouter:
    type: "openrouter"
    # api_key should be set via environment variable OPENROUTER_API_KEY

# =====================
# MODEL PRESETS
# =====================
model_presets:
  gemma2s:
    base_model: "google/gemma-2-2b-it"
    sparse_model: "../models/gemmascope-sparsify-1m"
    explainer_model: "meta-llama/Llama-3.1-8B-Instruct"
  gemma2m:
    base_model: "google/gemma-2-2b-it"
    sparse_model: "../models/gemmascope-sparsify-1m"
    explainer_model: "Qwen/Qwen3-32B-AWQ"
  gemma2b:
    base_model: "meta-llama/Meta-Llama-3-8B"
    sparse_model: "EleutherAI/sae-llama-3-8b-32x"
    explainer_model: "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4"

# =====================
# EXPERIMENT PRESETS
# =====================
experiment_presets:
  quick_test:
    max_latents: 5
    sampler_config: "default"
    constructor_config: "default"
    explainer: "default"
    client: "offline"
    model_preset: "gemma2s"
    hookpoints: ["layers.5"]

  graph:
    max_latents: 50
    sampler_config: "default"
    constructor_config: "default"
    explainer: "graph"
    client: "offline"
    model_preset: "gemma2m"
    hookpoints: ["layers.5"]

# =====================
# CACHE GENERATION SETTINGS
# =====================
cache_generation:
  default:
    n_tokens: 1000000
    batch_size: 16
    ctx_len: 256
    n_splits: 5
    dataset_repo: "EleutherAI/fineweb-edu-dedup-10b"
    dataset_split: "train[:1%]"
    dataset_column: "text"
    load_in_8bit: false
    filter_bos: true
