{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring explanations after generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will show a simple example of how to score the explanations generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OpenRouter' from 'sae_auto_interp.clients' (/mnt/ssd-1/gpaulo/SAE-Zoology/sae_auto_interp/clients/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msae_auto_interp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclients\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenRouter\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msae_auto_interp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExperimentConfig, FeatureConfig\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msae_auto_interp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultExplainer\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'OpenRouter' from 'sae_auto_interp.clients' (/mnt/ssd-1/gpaulo/SAE-Zoology/sae_auto_interp/clients/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
<<<<<<< HEAD
    "from os import getenv\n",
    "\n",
    "API_KEY = getenv(\"OPENROUTER_API_KEY\")\n",
    "import torch\n",
    "import orjson\n",
=======
    "\n",
    "API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
>>>>>>> e4bb340... Run ruff, start integrating scorer
    "import asyncio\n",
    "\n",
    "import orjson\n",
    "import torch\n",
    "\n",
    "from sae_auto_interp.clients import OpenRouter\n",
    "from sae_auto_interp.config import ExperimentConfig, FeatureConfig\n",
    "from sae_auto_interp.explainers import explanation_loader\n",
    "from sae_auto_interp.features import FeatureDataset, FeatureLoader\n",
    "from sae_auto_interp.features.constructors import default_constructor\n",
    "from sae_auto_interp.features.samplers import sample\n",
    "from sae_auto_interp.pipeline import Pipeline, process_wrapper\n",
    "from sae_auto_interp.scorers import FuzzingScorer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cfg = FeatureConfig(\n",
    "    width=131072, # The number of latents of your SAE\n",
    "    min_examples=200, # The minimum number of examples to consider for the feature to be explained\n",
    "    max_examples=10000, # The maximum number of examples to be sampled from\n",
    "    n_splits=5 # How many splits was the cache split into\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = \".model.layers.10\" # The layer to score\n",
    "feature_dict = {module: torch.arange(0,100)} # The what latents to score\n",
    "\n",
    "dataset = FeatureDataset(\n",
    "        raw_dir=\"raw_features\", # The folder where the cache is stored\n",
    "        cfg=feature_cfg,\n",
    "        modules=[module],\n",
    "        features=feature_dict,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the config for the examples shown to the scorer model.\n",
    "When selecting the examples to be shown to the scorer model we can select them from:\n",
    "- \"quantiles\", which gets examples from the quantiles of the data\n",
    "- \"activations\", which gets examples from activation bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_cfg = ExperimentConfig(\n",
    "    n_examples_test=20, # Number of examples to sample for testing\n",
    "    n_quantiles=10, # Number of quantiles to sample\n",
    "    example_ctx_len=32, # Length of each example\n",
    "    test_type=\"quantiles\", # Type of sampler to use for testing. \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor and sampler here are the same as the ones used in the generation of the explanations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'partial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m constructor\u001b[38;5;241m=\u001b[39m\u001b[43mpartial\u001b[49m(\n\u001b[1;32m      2\u001b[0m             default_constructor,\n\u001b[1;32m      3\u001b[0m             tokens\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtokens,\n\u001b[1;32m      4\u001b[0m             n_random\u001b[38;5;241m=\u001b[39mexperiment_cfg\u001b[38;5;241m.\u001b[39mn_random, \n\u001b[1;32m      5\u001b[0m             ctx_len\u001b[38;5;241m=\u001b[39mexperiment_cfg\u001b[38;5;241m.\u001b[39mexample_ctx_len, \n\u001b[1;32m      6\u001b[0m             max_examples\u001b[38;5;241m=\u001b[39mfeature_cfg\u001b[38;5;241m.\u001b[39mmax_examples\n\u001b[1;32m      7\u001b[0m         )\n\u001b[1;32m      8\u001b[0m sampler\u001b[38;5;241m=\u001b[39mpartial(sample,cfg\u001b[38;5;241m=\u001b[39mexperiment_cfg)\n\u001b[1;32m      9\u001b[0m loader \u001b[38;5;241m=\u001b[39m FeatureLoader(dataset, constructor\u001b[38;5;241m=\u001b[39mconstructor, sampler\u001b[38;5;241m=\u001b[39msampler)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'partial' is not defined"
     ]
    }
   ],
   "source": [
    "constructor=partial(\n",
    "            default_constructor,\n",
    "            tokens=dataset.tokens,\n",
    "            n_random=experiment_cfg.n_random, \n",
    "            ctx_len=experiment_cfg.example_ctx_len, \n",
    "            max_examples=feature_cfg.max_examples\n",
    "        )\n",
    "sampler=partial(sample,cfg=experiment_cfg)\n",
    "loader = FeatureLoader(dataset, constructor=constructor, sampler=sampler)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we could generate the explanations in the pipeline, here we load the explanations already generated. Then we define the scorer. Because the scorer should use information from the previous pipe, we have a preprocess and a postprocess function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenRouter(\"anthropic/claude-3.5-sonnet\",api_key=API_KEY)\n",
    "\n",
    "# Load the explanations already generated\n",
    "explainer_pipe = partial(explanation_loader, explanation_dir=\"results/explanations\")\n",
    "\n",
    "\n",
    "# Builds the record from result returned by the pipeline\n",
    "def scorer_preprocess(result):\n",
    "        record = result.record   \n",
    "        record.explanation = result.explanation\n",
    "        record.extra_examples = record.random_examples\n",
    "\n",
    "        return record\n",
    "\n",
    "# Saves the score to a file\n",
    "def scorer_postprocess(result, score_dir):\n",
    "    with open(f\"results/scores/{result.record.feature}.txt\", \"wb\") as f:\n",
    "        f.write(orjson.dumps(result.score))\n",
    "\n",
    "\n",
    "scorer_pipe = process_wrapper(\n",
    "    FuzzingScorer(client, tokenizer=dataset.tokenizer),\n",
    "    preprocess=scorer_preprocess,\n",
    "    postprocess=partial(scorer_postprocess, score_dir=\"fuzz\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our pipeline only has three steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    loader,\n",
    "    explainer_pipe,\n",
    "    scorer_pipe,\n",
    ")\n",
    "number_of_parallel_latents = 10\n",
    "asyncio.run(pipeline.run(number_of_parallel_latents)) # This will start generating the explanations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
