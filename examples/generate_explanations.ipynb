{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating explanations after caching the latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will show a simple example of how to generate explanations for a SAE after caching the latents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import orjson\n",
    "import torch\n",
    "\n",
    "from delphi.clients import OpenRouter\n",
    "from delphi.config import ExperimentConfig, LatentConfig\n",
    "from delphi.explainers import DefaultExplainer\n",
    "from delphi.latents import LatentDataset\n",
    "from delphi.latents.constructors import default_constructor\n",
    "from delphi.latents.samplers import sample\n",
    "from delphi.pipeline import Pipeline, process_wrapper\n",
    "\n",
    "API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_cfg = LatentConfig(\n",
    "    width=131072, # The number of latents of your SAE\n",
    "    min_examples=200, # The minimum number of examples to consider for the latent to be explained\n",
    "    max_examples=10000, # The maximum number of examples to be sampled from\n",
    "    n_splits=5 # How many splits was the cache split into\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = \".model.layers.10\" # The layer to explain\n",
    "latent_dict = {module: torch.arange(0,5)} # The what latents to explain\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the config for the examples shown to the explainer model.\n",
    "When selecting the examples to be shown to the explainer model we can select them from:\n",
    "- \"top\", which gets the most activating examples\n",
    "- \"random\" which gets random examples from the whole activation distribution\n",
    "- \"quantiles\" which gets examples from the quantiles of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_cfg = ExperimentConfig(\n",
    "    n_examples_train=40, # Number of examples to sample for training\n",
    "    example_ctx_len=32, # Length of each example\n",
    "    train_type=\"quantiles\", # Type of sampler to use for training. \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor defines the window of tokens to be used for the examples. We have a default constructor that builds examples of size ctx_len (should be a divisor of the ctx_len used for caching the latents).\n",
    "The sampler defines how the examples are selected. The sampler will always generate a train and test set, but here we only care about the train set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor=partial(\n",
    "            default_constructor,\n",
    "            token_loader=None,\n",
    "            n_not_active=experiment_cfg.n_non_activating, \n",
    "            ctx_len=experiment_cfg.example_ctx_len, \n",
    "            max_examples=latent_cfg.max_examples\n",
    "        )\n",
    "sampler=partial(sample,cfg=experiment_cfg)\n",
    "dataset = LatentDataset(\n",
    "        raw_dir=\"latents\", # The folder where the cache is stored\n",
    "        cfg=latent_cfg,\n",
    "        modules=[module],\n",
    "        latents=latent_dict,\n",
    "        constructor=constructor,\n",
    "        sampler=sampler\n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pipes to generate the explanations. Each pipe starts with loading the examples from the corresponding latent and then passes the examples to the explainer. It used a client (here OpenRouter) to generate the explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenRouter(\"anthropic/claude-3.5-sonnet\",api_key=API_KEY)\n",
    "\n",
    "# The function that saves the explanations\n",
    "def explainer_postprocess(result):\n",
    "        with open(f\"results/explanations/{result.record.latent}.txt\", \"wb\") as f:\n",
    "            f.write(orjson.dumps(result.explanation))\n",
    "        del result\n",
    "        return None\n",
    "\n",
    "explainer_pipe = process_wrapper(\n",
    "        DefaultExplainer(\n",
    "            client, \n",
    "            tokenizer=dataset.tokenizer,\n",
    "        ),\n",
    "        postprocess=explainer_postprocess,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p results/explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are generating only explanations, so our pipeline only has two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing items: 0it [01:20, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No available randomly sampled non-activating sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing items: 3it [00:04,  1.61s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    dataset,\n",
    "    explainer_pipe,\n",
    ")\n",
    "number_of_parallel_latents = 10\n",
    "await pipeline.run(number_of_parallel_latents) # This will start generating the explanations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
