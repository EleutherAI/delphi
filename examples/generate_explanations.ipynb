{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating explanations after caching the latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will show a simple example of how to generate explanations for a SAE after caching the latents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "from functools import partial\n",
    "from os import getenv\n",
    "\n",
    "API_KEY = getenv(\"OPENROUTER_API_KEY\")\n",
    "import torch\n",
    "import orjson\n",
=======
    "import os\n",
    "from functools import partial\n",
    "\n",
    "API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
>>>>>>> e4bb340... Run ruff, start integrating scorer
    "import asyncio\n",
    "\n",
    "import orjson\n",
    "import torch\n",
    "\n",
    "from sae_auto_interp.clients import OpenRouter\n",
    "from sae_auto_interp.config import ExperimentConfig, FeatureConfig\n",
    "from sae_auto_interp.explainers import DefaultExplainer\n",
    "from sae_auto_interp.features import FeatureDataset, FeatureLoader\n",
    "from sae_auto_interp.features.constructors import default_constructor\n",
    "from sae_auto_interp.features.samplers import sample\n",
    "from sae_auto_interp.pipeline import Pipeline, process_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cfg = FeatureConfig(\n",
    "    width=131072, # The number of latents of your SAE\n",
    "    min_examples=200, # The minimum number of examples to consider for the feature to be explained\n",
    "    max_examples=10000, # The maximum number of examples to be sampled from\n",
    "    n_splits=5 # How many splits was the cache split into\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/rpj-v2-sample  train[:1%]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137b647dfef646aeb58160056aca7b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "module = \".model.layers.10\" # The layer to explain\n",
    "feature_dict = {module: torch.arange(0,100)} # The what latents to explain\n",
    "\n",
    "dataset = FeatureDataset(\n",
    "        raw_dir=\"latents\", # The folder where the cache is stored\n",
    "        cfg=feature_cfg,\n",
    "        modules=[module],\n",
    "        features=feature_dict,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the config for the examples shown to the explainer model.\n",
    "When selecting the examples to be shown to the explainer model we can select them from:\n",
    "- \"top\", which gets the most activating examples\n",
    "- \"random\" which gets random examples from the whole activation distribution\n",
    "- \"quantiles\" which gets examples from the quantiles of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_cfg = ExperimentConfig(\n",
    "    n_examples_train=40, # Number of examples to sample for training\n",
    "    example_ctx_len=32, # Length of each example\n",
    "    train_type=\"random\", # Type of sampler to use for training. \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor defines the window of tokens to be used for the examples. We have a default constructor that builds examples of size ctx_len (should be a divisor of the ctx_len used for caching the latents).\n",
    "The sampler defines how the examples are selected. The sampler will always generate a train and test set, but here we only care about the train set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor=partial(\n",
    "            default_constructor,\n",
    "            tokens=dataset.tokens,\n",
    "            n_random=experiment_cfg.n_random, \n",
    "            ctx_len=experiment_cfg.example_ctx_len, \n",
    "            max_examples=feature_cfg.max_examples\n",
    "        )\n",
    "sampler=partial(sample,cfg=experiment_cfg)\n",
    "loader = FeatureLoader(dataset, constructor=constructor, sampler=sampler)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pipes to generate the explanations. Each pipe starts with loading the examples from the corresponding latent and then passes the examples to the explainer. It used a client (here OpenRouter) to generate the explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenRouter(\"anthropic/claude-3.5-sonnet\",api_key=API_KEY)\n",
    "\n",
    "# The function that saves the explanations\n",
    "def explainer_postprocess(result):\n",
    "        with open(f\"results/explanations/{result.record.feature}.txt\", \"wb\") as f:\n",
    "            f.write(orjson.dumps(result.explanation))\n",
    "        del result\n",
    "        return None\n",
    "\n",
    "explainer_pipe = process_wrapper(\n",
    "        DefaultExplainer(\n",
    "            client, \n",
    "            tokenizer=dataset.tokenizer,\n",
    "        ),\n",
    "        postprocess=explainer_postprocess,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are generating only explanations, show our pipeline only has two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Pipeline.run at 0x75acc5981770>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    loader,\n",
    "    explainer_pipe,\n",
    ")\n",
    "number_of_parallel_latents = 10\n",
    "asyncio.run(pipeline.run(number_of_parallel_latents)) # This will start generating the explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
